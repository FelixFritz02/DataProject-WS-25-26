{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f95383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"preprocessed_apartment_rentals_one_hot_encoded_states.csv\")\n",
    "\n",
    "# Features und Ziel\n",
    "X = data.drop(columns=[\"price\", \"state\"])\n",
    "y = data[\"price\"]\n",
    "scaler = StandardScaler()\n",
    "y = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc27c4",
   "metadata": {},
   "source": [
    "## ðŸ§  Hyperparameter des MLPRegressor (Neuronales Netz)\n",
    "\n",
    "Der **MLPRegressor** implementiert ein Feedforward-Neuronales Netz und ist ein komplexes nichtlineares Modell. Die Parameter definieren das Suchgitter (`mlp_params`) fÃ¼r die Optimierung der Architektur und Regulierung in der Nested Cross-Validation.\n",
    "\n",
    "| Parameter | Beschreibung | Im Code definierte Werte |\n",
    "| :--- | :--- | :--- |\n",
    "| **`hidden_layer_sizes`** | Definiert die **Architektur des Netzes**: Die Anzahl der versteckten Schichten (Hidden Layers) und die Anzahl der Neuronen pro Schicht. | `[(10, 10), (50, 50)]` |\n",
    "| **`activation`** | Die **Aktivierungsfunktion** fÃ¼r die versteckten Schichten. Definiert die NichtlinearitÃ¤t des Modells. | `[\"relu\"]` |\n",
    "| **`alpha`** | Der **L2-Regularisierungsterm** (Ã¤quivalent zu Ridge Regression). Ein hÃ¶herer Wert reduziert die KomplexitÃ¤t und hilft, **Overfitting** zu verhindern. | `np.logspace(-5, -2, 4)` |\n",
    "| **`max_iter`** | Die **maximale Anzahl von Iterationen** (Epochen), die der Optimierer Ã¼ber die Trainingsdaten ausfÃ¼hren soll. | `[500]` |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ ErlÃ¤uterung der Netzarchitektur\n",
    "\n",
    "\n",
    "\n",
    "[Image of a feedforward neural network architecture showing input, hidden, and output layers]\n",
    "\n",
    "\n",
    "* **Netzarchitektur:** `(10, 10)` bedeutet zwei versteckte Schichten mit jeweils 10 Neuronen. `(50, 50)` bedeutet zwei versteckte Schichten mit jeweils 50 Neuronen.\n",
    "* **`relu` (Rectified Linear Unit):** Sorgt dafÃ¼r, dass das Netz **NichtlinearitÃ¤ten** lernen kann (d.h. komplexere Beziehungen als eine einfache lineare Regression).\n",
    "* **`alpha` (Regularisierung):** Die Suche Ã¼ber $\\text{np.logspace}(-5, -2, 4)$ testet logarithmisch verteilte Werte zwischen $10^{-5}$ und $10^{-2}$. Dies ist entscheidend, um die ModellkomplexitÃ¤t zu kontrollieren.\n",
    "* **`max_iter`:** Da Neuronale Netze sequenziell lernen, ist ein ausreichender Wert notwendig, um die **Konvergenz** des Optimierers zu gewÃ¤hrleisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36354f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte MLP Nested CV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 4.8min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.4min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 3.2min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.8min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 3.8min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 3.5min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 3.7min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 4.1min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.5min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 4.5min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.7min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 1.0min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.3min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.1min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 3.4min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.0min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.1min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 1.9min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 1.8min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.0min\n",
      "Outer Fold 1/5 | Best Params: {'batch_size': 16, 'early_stopping': True, 'hidden_layer_sizes': (50, 50), 'max_iter': 1000} | Fit Time: 163.728s | Outer RÂ²: 0.689 | MSE: 147337.391\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 5.0min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.4min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 3.7min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.9min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.6min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.9min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.5min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 3.9min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 4.3min\n",
      "[CV] END batch_size=16, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 3.9min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.5min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time=  52.8s\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 1.8min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.9min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(10, 10), max_iter=1000; total time= 2.5min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.6min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 3.9min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.3min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 1.5min\n",
      "[CV] END batch_size=32, early_stopping=True, hidden_layer_sizes=(50, 50), max_iter=1000; total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 2/5 | Best Params: {'batch_size': 16, 'early_stopping': True, 'hidden_layer_sizes': (50, 50), 'max_iter': 1000} | Fit Time: 246.295s | Outer RÂ²: 0.688 | MSE: 145058.973\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "from Nested_CV import NestedCVRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_params = {\n",
    "    \"hidden_layer_sizes\": [(10,10), (50, 50)],\n",
    "    #\"activation\": [\"relu\"],\n",
    "    #\"alpha\": np.logspace(-5, -2, 4),\n",
    "    \"max_iter\": [1000], # ErhÃ¶hen der Iterationen ist oft nÃ¶tig\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"early_stopping\": [True]\n",
    "}\n",
    "\n",
    "mlp_cv = NestedCVRegressor(MLPRegressor(random_state=42), mlp_params)\n",
    "print(\"Starte MLP Nested CV...\")\n",
    "mlp_cv.run(X, y, output=True)\n",
    "\n",
    "print(\"\\n--- MLP Ergebnisse ---\")\n",
    "print(\"MLP Mean RÂ²:\", mlp_cv.get_mean_r2())\n",
    "print(\"MLP best params:\", mlp_cv.get_best_params())\n",
    "#mlp_cv.plot_scores(\"MLP Nested CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp_cv.get_best_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "number_outer_cv_splits = 5\n",
    "model_evaluation = pd.DataFrame({\"Fold\": range(1, number_outer_cv_splits+1)})\n",
    "model_evaluation['Modell'] = ['MLP_NN'] * number_outer_cv_splits\n",
    "model_evaluation['R_2'] = mlp_cv.get_r2_scores()\n",
    "model_evaluation['MSE'] = mlp_cv.get_mse_scores()\n",
    "model_evaluation[\"runtime\"] = mlp_cv.get_fit_times()\n",
    "model_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016d396",
   "metadata": {},
   "source": [
    "# TORCH NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d06fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class TorchRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, hidden_sizes=(64, 64), lr=1e-3, epochs=50, batch_size=64, device=None):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _build_model(self, input_dim):\n",
    "        layers = []\n",
    "        last = input_dim\n",
    "        for h in self.hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        self.model = self._build_model(X.shape[1]).to(self.device)\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for xb, yb in loader:\n",
    "                opt.zero_grad()\n",
    "                pred = self.model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        self.model.eval()\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.model(X).cpu().numpy().ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254e7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class TorchRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, hidden_sizes=(64, 64), lr=1e-3, epochs=50, batch_size=64, \n",
    "                 patience=None, device=None):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Listen fÃ¼r Loss-Tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def _build_model(self, input_dim):\n",
    "        layers = []\n",
    "        last = input_dim\n",
    "        for h in self.hidden_sizes:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        # 80/20 Split fÃ¼r Validation\n",
    "        split_idx = int(0.8 * len(X))\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        self.model = self._build_model(X_train.shape[1]).to(self.device)\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Reset loss tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_train_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            # Training\n",
    "            for xb, yb in loader:\n",
    "                opt.zero_grad()\n",
    "                pred = self.model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                \n",
    "                epoch_train_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            # Durchschnittlicher Training Loss fÃ¼r diese Epoch\n",
    "            avg_train_loss = epoch_train_loss / num_batches\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = self.model(X_val)\n",
    "                val_loss = loss_fn(val_pred, y_val).item()\n",
    "                self.val_losses.append(val_loss)\n",
    "            self.model.train()\n",
    "            \n",
    "            # Early stopping check (falls patience gesetzt)\n",
    "            if self.patience is not None:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}/{self.epochs}\")\n",
    "                        break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        self.model.eval()\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.model(X).cpu().numpy().ravel()\n",
    "    \n",
    "    def plot_losses(self, figsize=(10, 6), save_path=None):\n",
    "        \"\"\"\n",
    "        Plottet die Training- und Validation-Loss Entwicklung.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : tuple, optional\n",
    "            GrÃ¶ÃŸe der Figure (width, height)\n",
    "        save_path : str, optional\n",
    "            Pfad zum Speichern des Plots (z.B. 'loss_plot.png')\n",
    "        \"\"\"\n",
    "        if not self.train_losses:\n",
    "            print(\"Keine Loss-Daten verfÃ¼gbar. Bitte erst fit() aufrufen.\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        plt.plot(epochs, self.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        plt.plot(epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        \n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "        plt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=11)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Markiere beste Validation Loss\n",
    "        best_epoch = np.argmin(self.val_losses) + 1\n",
    "        best_val_loss = min(self.val_losses)\n",
    "        plt.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.5, \n",
    "                    label=f'Best Val Loss @ Epoch {best_epoch}')\n",
    "        plt.plot(best_epoch, best_val_loss, 'g*', markersize=15)\n",
    "        \n",
    "        plt.legend(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot gespeichert unter: {save_path}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114987d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Torch Nested CV...\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Early stopping at epoch 26/50\n",
      "Early stopping at epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 1/5 | Best Params: {'model__batch_size': 32, 'model__epochs': 50, 'model__hidden_sizes': (512, 256, 128, 32), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 15} | Fit Time: 311.466s | Outer RÂ²: 0.752 | MSE: 0.252 | Inner RÂ²: 0.797\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Early stopping at epoch 25/50\n",
      "Early stopping at epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 2/5 | Best Params: {'model__batch_size': 32, 'model__epochs': 50, 'model__hidden_sizes': (128, 128, 64), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 10} | Fit Time: 69.170s | Outer RÂ²: 0.747 | MSE: 0.251 | Inner RÂ²: 0.784\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Early stopping at epoch 21/50\n",
      "Early stopping at epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 3/5 | Best Params: {'model__batch_size': 64, 'model__epochs': 50, 'model__hidden_sizes': (512, 256, 128, 32), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 15} | Fit Time: 118.191s | Outer RÂ²: 0.752 | MSE: 0.248 | Inner RÂ²: 0.799\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Early stopping at epoch 21/50\n",
      "Early stopping at epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 4/5 | Best Params: {'model__batch_size': 32, 'model__epochs': 50, 'model__hidden_sizes': (512, 256, 128, 32), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 15} | Fit Time: 192.429s | Outer RÂ²: 0.745 | MSE: 0.254 | Inner RÂ²: 0.805\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Early stopping at epoch 13/50\n",
      "Early stopping at epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 5/5 | Best Params: {'model__batch_size': 32, 'model__epochs': 50, 'model__hidden_sizes': (128, 128, 64), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 5} | Fit Time: 51.732s | Outer RÂ²: 0.750 | MSE: 0.249 | Inner RÂ²: 0.779\n",
      "\n",
      "--- Torch NN Ergebnisse ---\n",
      "Mean RÂ²: 0.7493031497864748\n",
      "Best params: [{'model__batch_size': 32, 'model__epochs': 50, 'model__hidden_sizes': (512, 256, 128, 32), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 15}, {'model__batch_size': 32, 'model__epochs': 50, 'model__hidden_sizes': (128, 128, 64), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 10}, {'model__batch_size': 64, 'model__epochs': 50, 'model__hidden_sizes': (512, 256, 128, 32), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 15}, {'model__batch_size': 32, 'model__epochs': 50, 'model__hidden_sizes': (512, 256, 128, 32), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 15}, {'model__batch_size': 32, 'model__epochs': 50, 'model__hidden_sizes': (128, 128, 64), 'model__lr': np.float64(0.0005623413251903491), 'model__patience': 5}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Nested_CV_targetEncoding import NestedCVRegressorWithTargetEncoding\n",
    "\n",
    "torch_params = {\n",
    "    \"hidden_sizes\": [(128, 128, 64),(512, 256, 128, 32)], # <- das war schon ganz ok R^2 0.74\n",
    "    \"lr\": np.logspace(-7, -2, 5),\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"epochs\": [50],\n",
    "    \"patience\": [5, 10, 15, None] # Early Stopping Geduld\n",
    "}\n",
    "\n",
    "torch_cv = NestedCVRegressorWithTargetEncoding(TorchRegressor(), param_grid=torch_params, encode_cols=[\"cityname\"], outer_splits=5, inner_splits=5)\n",
    "\n",
    "print(\"Starte Torch Nested CV...\")\n",
    "torch_cv.run(X, y, output=True)\n",
    "\n",
    "print(\"\\n--- Torch NN Ergebnisse ---\")\n",
    "print(\"Mean RÂ²:\", torch_cv.get_mean_r2())\n",
    "print(\"Best params:\", torch_cv.get_best_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720aaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03123c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Modell</th>\n",
       "      <th>R_2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Torch_NN</td>\n",
       "      <td>0.752059</td>\n",
       "      <td>0.251521</td>\n",
       "      <td>311.465810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Torch_NN</td>\n",
       "      <td>0.747429</td>\n",
       "      <td>0.251286</td>\n",
       "      <td>69.170410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Torch_NN</td>\n",
       "      <td>0.751573</td>\n",
       "      <td>0.248088</td>\n",
       "      <td>118.190902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Torch_NN</td>\n",
       "      <td>0.744980</td>\n",
       "      <td>0.253895</td>\n",
       "      <td>192.428898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Torch_NN</td>\n",
       "      <td>0.750475</td>\n",
       "      <td>0.248586</td>\n",
       "      <td>51.732040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold    Modell       R_2       MSE     runtime\n",
       "0     1  Torch_NN  0.752059  0.251521  311.465810\n",
       "1     2  Torch_NN  0.747429  0.251286   69.170410\n",
       "2     3  Torch_NN  0.751573  0.248088  118.190902\n",
       "3     4  Torch_NN  0.744980  0.253895  192.428898\n",
       "4     5  Torch_NN  0.750475  0.248586   51.732040"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "number_outer_cv_splits = 5\n",
    "model_evaluation = pd.DataFrame({\"Fold\": range(1, number_outer_cv_splits+1)})\n",
    "model_evaluation['Modell'] = ['Torch_NN'] * number_outer_cv_splits\n",
    "model_evaluation['R_2'] = torch_cv.get_r2_scores()\n",
    "model_evaluation['MSE'] = torch_cv.get_mse_scores()\n",
    "model_evaluation[\"runtime\"] = torch_cv.get_fit_times()\n",
    "model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83059ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation.to_pickle('benchmarking/TorchNN.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
